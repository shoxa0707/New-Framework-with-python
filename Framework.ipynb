{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13559367",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a0e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, inp=1, out=1):\n",
    "        self.in_neuron = inp\n",
    "        self.out_neuron = out\n",
    "        self.create_parameters()\n",
    "    \n",
    "    def create_parameters(self):\n",
    "        self.weights = np.random.uniform(-1, 1, (self.in_neuron, self.out_neuron))\n",
    "        self.bias = np.random.uniform(-1, 1, (1, self.out_neuron))\n",
    "        \n",
    "    def set_parameters(self, x):\n",
    "        self.weights = np.array(x['weights'])\n",
    "        self.bias = np.array(x['bias'])\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return [self.weights, self.bias]\n",
    "    \n",
    "    def get_data(self):\n",
    "        return [self.in_neuron, self.out_neuron, self.name]\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        if isinstance(x, list):\n",
    "            self.input = x[0]\n",
    "        else:\n",
    "            self.input = x\n",
    "        out = [np.dot(self.input, self.weights) + self.bias, self.name]\n",
    "        self.output = out.copy()\n",
    "        return out\n",
    "        \n",
    "class Activations:\n",
    "    def __init__(self,act_name='relu'):\n",
    "        if act_name == 'relu':\n",
    "            self.active_func = self.relu\n",
    "            self.derivate = self.der_relu\n",
    "        elif act_name == 'sigmoid':\n",
    "            self.active_func = self.sigmoid\n",
    "            self.derivate = self.der_sigmoid\n",
    "        elif act_name == 'softmax':\n",
    "            self.active_func = self.softmax\n",
    "            self.derivate = self.der_softmax\n",
    "        else:\n",
    "            print('bunaqa activation function bizda yo\\'q')\n",
    "        self.output = []\n",
    "        self.name = act_name\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self.name\n",
    "    \n",
    "    def relu(self, xname):\n",
    "        x = xname[0]\n",
    "        out = [np.maximum(0,x), xname[1], self.name]\n",
    "        self.output.append(out.copy())\n",
    "        return out\n",
    "    \n",
    "    def der_relu(self, a):\n",
    "        x = a.copy()\n",
    "        x[x<=0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "    \n",
    "    def sigmoid(self,xname):\n",
    "        x = xname[0]\n",
    "        out = [1/(1+np.exp(-x)), xname[1], self.name]\n",
    "        self.output.append(out.copy())\n",
    "        return out\n",
    "    \n",
    "    def der_sigmoid(self, x):\n",
    "        return x*(1-x)\n",
    "    \n",
    "    def softmax(self, xname):\n",
    "        x = xname[0]\n",
    "        out = [np.exp(x)/np.sum(np.exp(x)), xname[1], self.name]\n",
    "        self.output.append(out.copy())\n",
    "        return out\n",
    "    \n",
    "    def der_softmax(self, x):\n",
    "        return np.diag(x) - np.outer(x, x)\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        return self.active_func(x)\n",
    "    \n",
    "class Loss:\n",
    "    def __init__(self, loss_name='MSE'):\n",
    "        if loss_name == 'MSE':\n",
    "            self.loss_func = self.MSE\n",
    "            self.derivate = self.der_MSE\n",
    "        elif loss_name == 'MAE':\n",
    "            self.loss_func = self.MAE\n",
    "            self.derivate = self.der_MAE\n",
    "        elif loss_name == 'cross_entropy_loss':\n",
    "            self.loss_func = self.CEL\n",
    "            self.derivate = self.der_CEL\n",
    "        else:\n",
    "            print('bunaqa loss bizda yo\\'q')\n",
    "        self.name = loss_name\n",
    "    \n",
    "    def MSE(self,target, predict):\n",
    "        return (target-predict)**2\n",
    "    \n",
    "    def der_MSE(self, target, predict):\n",
    "        return -2*(target-predict)\n",
    "    \n",
    "    def MAE(self, target, predict):\n",
    "        return abs(target-predict)\n",
    "    \n",
    "    def der_MAE(self, target, predict):\n",
    "        if target > predict:\n",
    "            return -1\n",
    "        elif target == predict:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def CEL(self, target, predict):\n",
    "        return -np.sum(np.multiply(target,np.log(predict)))\n",
    "    \n",
    "    def der_CEL(self, target, predict):\n",
    "        return predict - target\n",
    "    \n",
    "    def __call__(self, target, output):\n",
    "        return self.loss_func(target, output)\n",
    "\n",
    "class PyDahoShoxa(object):\n",
    "    params = {}\n",
    "    layers_list = []\n",
    "    activs_list = []\n",
    "    def __init__(self, layers=[]):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def compile(self, optimizer, loss):\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer.loss = Loss(loss)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.layers_out = [x]\n",
    "        for i in range(len(self.layers)):\n",
    "            x = np.dot(x, self.layers[i].weights) + self.layers[i].bias\n",
    "            self.layers_out.append(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.forward(x)[0][0]\n",
    "    \n",
    "    def parameters(self):\n",
    "        parameters = []\n",
    "        for i in self.layers_list:\n",
    "            parameters.append(self.params[i].get_parameters())\n",
    "        return parameters\n",
    "    \n",
    "    def set_parameters(self, updated_param):\n",
    "        for ind, i in enumerate(self.layers_list):\n",
    "            self.params[i].set_parameters(updated_param[ind])\n",
    "            \n",
    "    def get_outputs(self):\n",
    "        outs = []\n",
    "        for i in self.layers_list:\n",
    "            outs.append(self.params[i].output)\n",
    "        names = [i[1] for i in outs]\n",
    "        for i in self.activs_list:\n",
    "            for j in self.params[i].output:\n",
    "                if j[1] in names:\n",
    "                    ind = names.index(j[1])\n",
    "                    outs.pop(ind)\n",
    "                    outs.insert(ind, j)\n",
    "        out = outs.copy()\n",
    "        if len(self.sequence) == 0 and len(out) > 0:\n",
    "            for i in out:\n",
    "                for j in i[1:]:\n",
    "                    self.sequence.append(self.params[j].get_data())\n",
    "        return out\n",
    "    \n",
    "    def fit(self, x, y, epochs=10):\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(x)\n",
    "            losses = 0\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            for i in range(len(x)):\n",
    "                start = time.time()\n",
    "                self.forward([x[i]])\n",
    "                outputs = self.get_outputs()\n",
    "                outputs.insert(0, [[x[i]]])\n",
    "                self.optimizer.outputs = outputs.copy()\n",
    "                self.optimizer.backward(y[i])\n",
    "                losses += np.sum(self.optimizer.loss(y[i], outputs[-1][0][0]))\n",
    "                if i != len(x)-1:\n",
    "                    about = f\"\"\"{i+1}/{len(x)} [{int((i+1)/len(x)*30)*\"=\"}>{(29-int((i+1)/len(x)*30))*\".\"}] - {round(time.time()-start, 2)} s - mean loss {losses/(i+1)}\"\"\"\n",
    "                    print(about,end='\\r')\n",
    "                else:\n",
    "                    about = f\"\"\"{(i+1)}/{len(x)} [{30*\"=\"}] - {round(time.time()-start, 2)} s - mean loss {losses/(i+1)}\"\"\"\n",
    "                    print(about)\n",
    "                    \n",
    "    def evaluate(self, x, y):\n",
    "        start = time.time()\n",
    "        predict = self.forward(x)\n",
    "        return self.r2_score(y, predict[0])\n",
    "    \n",
    "    def r2_score(self, target, predict):\n",
    "        tar = target[:len(predict)]\n",
    "        residuals = np.sum(Loss('MSE')(tar, predict))\n",
    "        mean = np.mean(target)\n",
    "        total = np.sum(np.power(target - mean, 2))\n",
    "        return 1 - residuals/total\n",
    "            \n",
    "    def save(self, path):\n",
    "        model = {}\n",
    "        pivot = 1\n",
    "        for i in self.sequence:\n",
    "            if isinstance(i, list):\n",
    "                parameters = self.params[i[2]].get_parameters()\n",
    "                model[f\"Layer{pivot}\"] = {}\n",
    "                model[f\"Layer{pivot}\"]['weights'], model[f\"Layer{pivot}\"]['bias'] = parameters[0].tolist(), parameters[1].tolist()\n",
    "                pivot += 1\n",
    "                \n",
    "        model['Architecture'] = self.sequence\n",
    "        model['Compile'] = {}\n",
    "        model['Compile']['optimizer_learning_rate'] = self.optimizer.learning_rate\n",
    "        model['Compile']['loss'] = self.optimizer.loss.name\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(model, f, indent=4)\n",
    "    \n",
    "    def __setattr__(self, name, value):\n",
    "        self.params[name]=value\n",
    "        if isinstance(value, Layer):\n",
    "            if name not in self.layers_list:\n",
    "                self.params[name].name = name\n",
    "                self.layers_list.append(name)\n",
    "        elif isinstance(value, Activations):\n",
    "            if name not in self.activs_list:\n",
    "                self.params[name].name = name\n",
    "                self.activs_list.append(name)\n",
    "        super().__setattr__(name, value)\n",
    "        \n",
    "    def __new__(cls):\n",
    "        obj = super().__new__(cls)\n",
    "        obj.params = {}\n",
    "        obj.layers_list = []\n",
    "        obj.activs_list = []\n",
    "        obj.sequence = []\n",
    "        return obj\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "class Optimizer:\n",
    "    def __init__(self, params=[], outputs=[], actives=['relu','sigmoid','softmax'], learning_rate=0.001):\n",
    "        self.params = params\n",
    "        self.outputs = outputs.copy()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activations = {}\n",
    "        for i in actives:\n",
    "            self.activations[i] = Activations(i)\n",
    "    \n",
    "    def qism_gradient(self, order, satr, ustun):\n",
    "        if len(self.outputs[-1]) > 2:\n",
    "            if self.outputs[-1][2] == 'softmax':\n",
    "                if order == len(self.params) - 1:\n",
    "                    last_act = np.zeros((self.outputs[-1][0].shape))\n",
    "                    last_act[0, ustun] = 1.0\n",
    "                    return last_act\n",
    "                else:\n",
    "                    last_act = 1\n",
    "            else:\n",
    "                last_act = self.activations[self.outputs[-1][-1]].derivate(self.outputs[-1][0])\n",
    "        else:\n",
    "            last_act = 1\n",
    "        if order == len(self.params) - 1:\n",
    "            return last_act\n",
    "        else:\n",
    "            grad = self.params[order+1][0][[ustun]].copy()\n",
    "            for i in range(order+2, len(self.params)):\n",
    "                if len(self.outputs[i]) > 2:\n",
    "                    acts = self.activations[self.outputs[i][-1]].derivate(self.outputs[i][0])[0]\n",
    "                    acts_matrix = np.tile(acts, (self.params[i][0].shape[1], 1)).T\n",
    "                else:\n",
    "                    acts_matrix = np.ones(self.params[i][0].shape)\n",
    "                layer_next = np.multiply(self.params[i][0], acts_matrix)\n",
    "                grad = np.dot(grad, layer_next)\n",
    "            return np.multiply(grad, last_act)\n",
    "    \n",
    "    def backward(self, target):\n",
    "        common_der_loss = self.loss.derivate(target, self.outputs[-1][0][0])\n",
    "        # layerlar uchun sikl\n",
    "        for i in range(len(self.params)-1, -1, -1):\n",
    "            ###################################\n",
    "            #### weightlarni update qilish ####\n",
    "            ###################################\n",
    "            qism_grad = []\n",
    "            if len(self.outputs[i]) == 3 and i != len(self.params)-1 and self.outputs[i][2] != 'softmax':\n",
    "                act_common = self.activations[self.outputs[i][-1]].derivate(self.outputs[i][0])\n",
    "                act_common = np.tile(act_common, (self.params[i][0].shape[1], 1)).T\n",
    "            else:\n",
    "                act_common = np.ones(self.params[i][0].shape)\n",
    "            for satr in range(len(self.params[i][0])):\n",
    "                for ustun in range(len(self.params[i][0][satr])):\n",
    "                    try:\n",
    "                        umumiy_grad = np.dot(qism_grad[ustun], common_der_loss)*act_common[satr, ustun]*self.outputs[i][0][0][satr]\n",
    "                        self.params[i][0][satr,ustun] -= self.learning_rate*umumiy_grad\n",
    "                    except:\n",
    "                        qism_grad.append(self.qism_gradient(i, satr, ustun))\n",
    "                        umumiy_grad = np.dot(qism_grad[ustun], common_der_loss)*act_common[satr, ustun]*self.outputs[i][0][0][satr]\n",
    "                        self.params[i][0][satr,ustun] -= self.learning_rate*umumiy_grad\n",
    "            #################################\n",
    "            #### biaslarni update qilish ####\n",
    "            #################################\n",
    "            for bias in range(len(qism_grad)):\n",
    "                umumiy_grad = np.dot(qism_grad[ustun], common_der_loss)*act_common[satr, ustun]\n",
    "                self.params[i][1][0,bias] -= self.learning_rate*umumiy_grad\n",
    "\n",
    "        return self.params\n",
    "    \n",
    "    \n",
    "    \n",
    "def load_model(path):\n",
    "    model = PyDahoShoxa()\n",
    "    # read model parameters and architecture\n",
    "    with open(path) as f:\n",
    "        model_data = json.load(f)\n",
    "    model.sequence = model_data['Architecture']\n",
    "    # set layers and their parameters\n",
    "    lay = 1\n",
    "    for i in model.sequence:\n",
    "        if isinstance(i, list):\n",
    "            model.layers_list.append(i[2])\n",
    "            model.params[i[2]] = Layer(i[0], i[1])\n",
    "            model.params[i[2]].name = i[2]\n",
    "            model.params[i[2]].set_parameters(model_data[f'Layer{lay}'])\n",
    "            setattr(model, i[2], model.params[i[2]])\n",
    "            lay += 1\n",
    "        else:\n",
    "            model.activs_list.append(i)\n",
    "            model.params[i] = Activations(i)\n",
    "    model.compile(Optimizer(params=model.parameters(), learning_rate=model_data['Compile']['optimizer_learning_rate']), loss=model_data['Compile']['loss'])\n",
    "    # forward function for loaded model\n",
    "    def forward(x):\n",
    "        for i in model.sequence:\n",
    "            if isinstance(i, list):\n",
    "                x = model.params[i[2]](x)\n",
    "            else:\n",
    "                x = model.params[i](x)\n",
    "        return x\n",
    "    \n",
    "    model.forward = forward\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfeec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(PyDahoShoxa):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = Layer(784, 64)\n",
    "        self.l2 = Layer(64, 16)\n",
    "        self.l3 = Layer(16, 10)\n",
    "        self.relu = Activations('relu')\n",
    "        self.softmax = Activations('softmax')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.l1(x))\n",
    "        x = self.relu(self.l2(x))\n",
    "        x = self.softmax(self.l3(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "my_model = Model()\n",
    "my_model.compile(optimizer=Optimizer(params=my_model.parameters(), learning_rate=0.001), loss='cross_entropy_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(PyDahoShoxa):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = Layer(784, 64)\n",
    "        self.layer2 = Layer(64, 16)\n",
    "        self.layer3 = Layer(16, 1)\n",
    "        self.relu = Activations('relu')\n",
    "        self.sigmoid = Activations('sigmoid')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "my_model2 = Model2()\n",
    "my_model2.compile(optimizer=Optimizer(params=my_model2.parameters(), learning_rate=0.0001), loss='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4426470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist():\n",
    "    a = []\n",
    "    b = []\n",
    "    (train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "    for i in range(len(train_X)):\n",
    "        a.append(train_X[i].flatten()/255)\n",
    "    for i in range(len(test_X)):\n",
    "        b.append(test_X[i].flatten())\n",
    "    return np.array(a), train_y, np.array(b), test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11136e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx, trainy, testx, testy = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47680fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = np.zeros((trainy.size, trainy.max() + 1))\n",
    "onehot[np.arange(trainy.size), trainy] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a856075e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_model.fit(trainx[:100], onehot[:100], epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073388b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.array([0,1,0]), np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d65865",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model2.fit(trainx[:10], trainy[:10], epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e8c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model2.save('model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c80b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = []\n",
    "for i in range(len(testy)):\n",
    "#     print('predict:',np.argmax(my_model.predict(trainx[i])))\n",
    "#     print('target:', trainy[i])\n",
    "    a.append(np.argmax(my_model.predict(trainx[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26636f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = list(set(a))\n",
    "for i in b:\n",
    "    print(i, ':', a.count(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c15abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(trainy.ravel())\n",
    "b = list(set(data))\n",
    "for i in b:\n",
    "    print(i, ':', data.count(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76057749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(PyDahoShoxa):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = Layer(4, 3)\n",
    "#         self.layer2 = Layer(3, 3)\n",
    "        self.layer3 = Layer(3, 2)\n",
    "        self.layer4 = Layer(2, 1)\n",
    "        self.relu = Activations('relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer(x))\n",
    "#         x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "my_model2 = Model2()\n",
    "\n",
    "pars = [{'weights':[[1, 0.5, -0.2],\n",
    "                   [0.1,  0.3, 1],\n",
    "                   [0 ,  0.5, 0.9],\n",
    "                   [1.1 ,  0.2, -0.3]],\n",
    "  \n",
    "        'bias': [[ 1,  1.5 , -2]]},\n",
    "        \n",
    " {'weights':[[0.4 , 0.7],\n",
    "             [-1,  0.8],\n",
    "             [3,  -2.5]],\n",
    "  \n",
    "        'bias':[[-2.5 , 3]]},\n",
    "        \n",
    " {'weights':[[0.0],\n",
    "             [1.0]],\n",
    " 'bias':[[3.5]]}]\n",
    "\n",
    "my_model2.set_parameters(pars)\n",
    "\n",
    "my_model2.compile(optimizer=Optimizer(params=my_model2.parameters(), learning_rate=0.001), loss='MSE')\n",
    "\n",
    "my_model2.fit([[1,2,3,4]], [[7]], epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755857e5",
   "metadata": {},
   "source": [
    "## Mini-batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be02a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, inp=1, out=1):\n",
    "        self.in_neuron = inp\n",
    "        self.out_neuron = out\n",
    "        self.create_parameters()\n",
    "    \n",
    "    def create_parameters(self):\n",
    "        self.weights = np.random.uniform(-1, 1, (self.in_neuron, self.out_neuron))\n",
    "        self.bias = np.random.uniform(-1, 1, (1, self.out_neuron))\n",
    "        \n",
    "    def set_parameters(self, x):\n",
    "        self.weights = np.array(x['weights'])\n",
    "        self.bias = np.array(x['bias'])\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return [self.weights, self.bias]\n",
    "    \n",
    "    def get_data(self):\n",
    "        return [self.in_neuron, self.out_neuron, self.name]\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        if isinstance(x, list):\n",
    "            self.input = x[0]\n",
    "        else:\n",
    "            self.input = x\n",
    "        out = [np.dot(self.input, self.weights) + self.bias, self.name]\n",
    "        self.output = out.copy()\n",
    "        return out\n",
    "        \n",
    "class Activations:\n",
    "    def __init__(self,act_name='relu'):\n",
    "        if act_name == 'relu':\n",
    "            self.active_func = self.relu\n",
    "            self.derivate = self.der_relu\n",
    "        elif act_name == 'sigmoid':\n",
    "            self.active_func = self.sigmoid\n",
    "            self.derivate = self.der_sigmoid\n",
    "        elif act_name == 'softmax':\n",
    "            self.active_func = self.softmax\n",
    "            self.derivate = self.der_softmax\n",
    "        else:\n",
    "            print('bunaqa activation function bizda yo\\'q')\n",
    "        self.output = []\n",
    "        self.name = act_name\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self.name\n",
    "    \n",
    "    def relu(self, xname):\n",
    "        x = xname[0]\n",
    "        out = [np.maximum(0,x), xname[1], self.name]\n",
    "        self.output.append(out.copy())\n",
    "        return out\n",
    "    \n",
    "    def der_relu(self, a):\n",
    "        x = a.copy()\n",
    "        x[x<=0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "    \n",
    "    def sigmoid(self,xname):\n",
    "        x = xname[0]\n",
    "        out = [1/(1+np.exp(-x)), xname[1], self.name]\n",
    "        self.output.append(out.copy())\n",
    "        return out\n",
    "    \n",
    "    def der_sigmoid(self, x):\n",
    "        return x*(1-x)\n",
    "    \n",
    "    def softmax(self, xname):\n",
    "        x = xname[0]\n",
    "        soft = []\n",
    "        for i in x:\n",
    "            soft.append(np.exp(i)/np.sum(np.exp(i)))\n",
    "        out = [np.array(soft), xname[1], self.name]\n",
    "        self.output.append(out.copy())\n",
    "        return out\n",
    "    \n",
    "    def der_softmax(self, x):\n",
    "        return np.diag(x) - np.outer(x, x)\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        return self.active_func(x)\n",
    "    \n",
    "class Loss:\n",
    "    def __init__(self, loss_name='MSE'):\n",
    "        if loss_name == 'MSE':\n",
    "            self.loss_func = self.MSE\n",
    "            self.derivate = self.der_MSE\n",
    "        elif loss_name == 'MAE':\n",
    "            self.loss_func = self.MAE\n",
    "            self.derivate = self.der_MAE\n",
    "        elif loss_name == 'cross_entropy_loss':\n",
    "            self.loss_func = self.CEL\n",
    "            self.derivate = self.der_CEL\n",
    "        else:\n",
    "            print('bunaqa loss bizda yo\\'q')\n",
    "        self.name = loss_name\n",
    "    \n",
    "    def MSE(self,target, predict):\n",
    "        return (target-predict)**2\n",
    "    \n",
    "    def der_MSE(self, target, predict):\n",
    "        return -2*(target-predict)\n",
    "    \n",
    "    def MAE(self, target, predict):\n",
    "        return abs(target-predict)\n",
    "    \n",
    "    def der_MAE(self, target, predict):\n",
    "        if target > predict:\n",
    "            return -1\n",
    "        elif target == predict:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def CEL(self, target, predict):\n",
    "        return -np.sum(np.multiply(target,np.log(predict)))\n",
    "    \n",
    "    def der_CEL(self, target, predict):\n",
    "        return predict - target\n",
    "    \n",
    "    def __call__(self, target, output):\n",
    "        return self.loss_func(target, output)\n",
    "\n",
    "class PyDahoShoxa(object):\n",
    "    params = {}\n",
    "    layers_list = []\n",
    "    activs_list = []\n",
    "    def __init__(self, layers=[]):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def compile(self, optimizer, loss):\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer.loss = Loss(loss)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.layers_out = [x]\n",
    "        for i in range(len(self.layers)):\n",
    "            x = np.dot(x, self.layers[i].weights) + self.layers[i].bias\n",
    "            self.layers_out.append(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.forward(x)[0][0]\n",
    "    \n",
    "    def parameters(self):\n",
    "        parameters = []\n",
    "        for i in self.layers_list:\n",
    "            parameters.append(self.params[i].get_parameters())\n",
    "        return parameters\n",
    "    \n",
    "    def set_parameters(self, updated_param):\n",
    "        for ind, i in enumerate(self.layers_list):\n",
    "            self.params[i].set_parameters(updated_param[ind])\n",
    "            \n",
    "    def get_outputs(self):\n",
    "        outs = []\n",
    "        for i in self.layers_list:\n",
    "            outs.append(self.params[i].output)\n",
    "        names = [i[1] for i in outs]\n",
    "        for i in self.activs_list:\n",
    "            for j in self.params[i].output:\n",
    "                if j[1] in names:\n",
    "                    ind = names.index(j[1])\n",
    "                    outs.pop(ind)\n",
    "                    outs.insert(ind, j)\n",
    "        out = outs.copy()\n",
    "        if len(self.sequence) == 0 and len(out) > 0:\n",
    "            for i in out:\n",
    "                for j in i[1:]:\n",
    "                    self.sequence.append(self.params[j].get_data())\n",
    "        return out\n",
    "    \n",
    "    def fit(self, x, y, batch_size=4, epochs=10):\n",
    "        self.optimizer.batch_size = batch_size\n",
    "        count_batch = math.ceil(len(x)/batch_size)\n",
    "        for epoch in range(epochs):\n",
    "            jami = time.time()\n",
    "            np.random.shuffle(x)\n",
    "            acc = 0\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            batches = self.split_datas(x=x, y=y, batch_size=batch_size, count_batch=count_batch)\n",
    "            for ind, batch in enumerate(batches):\n",
    "                start = time.time()\n",
    "                self.forward(batch[0])\n",
    "                outputs = self.get_outputs()\n",
    "                outputs.insert(0, [batch[0]])\n",
    "                self.optimizer.outputs = outputs.copy()\n",
    "                self.optimizer.backward(batch[1])\n",
    "                loss = self.optimizer.loss(batch[1], outputs[-1][0][0])\n",
    "                if self.sequence[-1] == 'sigmoid' or self.optimizer.loss.name == 'cross_entropy_loss':\n",
    "                    acc += self.accuracy(batch[1], outputs[-1][0])\n",
    "                    acc /= len(x)\n",
    "                else:\n",
    "                    acc = self.accuracy(batch[1], outputs[-1][0])\n",
    "                if ind != count_batch - 1:\n",
    "                    eta = self.sec2user(math.floor((count_batch-ind-1)*(time.time()-start)))\n",
    "                    about = f\"\"\"{ind+1}/{count_batch} [{int((ind+1)/count_batch*30)*\"=\"}>{(29-int((ind+1)/count_batch*30))*\".\"}] - ETA: {eta} - loss: {round(loss, 4)} - accuracy: {round(acc, 4)}\"\"\"\n",
    "                    print(about,end='\\r')\n",
    "                else:\n",
    "                    jami = time.time()-jami\n",
    "                    about = f\"\"\"{(ind+1)}/{count_batch} [{30*\"=\"}] - {self.sec2user(math.floor(jami))} {math.floor(jami/count_batch)}ms/step - loss {round(loss, 4)} - accuracy: {round(acc, 4)}\"\"\"\n",
    "                    print(about)\n",
    "    \n",
    "    def sec2user(self, sec):\n",
    "        secs = str(sec % 60)\n",
    "        if sec < 60:\n",
    "            return secs + 's'\n",
    "        if len(secs) == 1:\n",
    "            secs = '0'+str(sec)\n",
    "        while True:\n",
    "            sec = sec // 60\n",
    "            if sec >= 60:\n",
    "                qism = sec% 60\n",
    "                if qism < 10:\n",
    "                    secs = '0'+str(qism)+':'+secs\n",
    "                else:\n",
    "                    secs = str(qism)+':'+secs\n",
    "            else:\n",
    "                if sec < 10:\n",
    "                    secs = '0'+str(sec)+':'+secs\n",
    "                else:\n",
    "                    secs = str(sec)+':'+secs\n",
    "                break\n",
    "        return secs\n",
    "    \n",
    "    def split_datas(self, x, y, batch_size=4, count_batch=0):\n",
    "        self.optimizer.batch_size = batch_size\n",
    "        datas = [(x[i],y[i]) for i in range(len(x))]\n",
    "        random.shuffle(datas)\n",
    "        for i in range(count_batch):\n",
    "            a = datas[i*batch_size:(i+1)*batch_size]\n",
    "            yield np.array([k[0] for k in a]), np.array([k[1] for k in a])\n",
    "                    \n",
    "    def evaluate(self, x, y):\n",
    "        start = time.time()\n",
    "        predict = self.forward(x)\n",
    "        return self.r2_score(y, predict[0])\n",
    "    \n",
    "    def accuracy(self, target, predict):\n",
    "        if self.optimizer.loss.name == 'cross_entropy_loss':\n",
    "            predict = np.argmax(predict, axis=1)\n",
    "            return np.sum(target==predict)\n",
    "        elif self.sequence[-1] == 'sigmoid':\n",
    "            predict[predict>=0.5] = 1\n",
    "            predict[predict<0.5] = 0\n",
    "            return np.sum(target==predict)\n",
    "        else:\n",
    "            self.r2_score(target, predict)\n",
    "    \n",
    "    def r2_score(self, target, predict):\n",
    "        tar = target[:len(predict)]\n",
    "        residuals = np.sum(Loss('MSE')(tar, predict))\n",
    "        mean = np.mean(target)\n",
    "        total = np.sum(np.power(target - mean, 2))\n",
    "        return 1 - residuals/total\n",
    "            \n",
    "    def save(self, path):\n",
    "        model = {}\n",
    "        pivot = 1\n",
    "        for i in self.sequence:\n",
    "            if isinstance(i, list):\n",
    "                parameters = self.params[i[2]].get_parameters()\n",
    "                model[f\"Layer{pivot}\"] = {}\n",
    "                model[f\"Layer{pivot}\"]['weights'], model[f\"Layer{pivot}\"]['bias'] = parameters[0].tolist(), parameters[1].tolist()\n",
    "                pivot += 1\n",
    "                \n",
    "        model['Architecture'] = self.sequence\n",
    "        model['Compile'] = {}\n",
    "        model['Compile']['optimizer_learning_rate'] = self.optimizer.learning_rate\n",
    "        model['Compile']['loss'] = self.optimizer.loss.name\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(model, f, indent=4)\n",
    "    \n",
    "    def __setattr__(self, name, value):\n",
    "        self.params[name]=value\n",
    "        if isinstance(value, Layer):\n",
    "            if name not in self.layers_list:\n",
    "                self.params[name].name = name\n",
    "                self.layers_list.append(name)\n",
    "        elif isinstance(value, Activations):\n",
    "            if name not in self.activs_list:\n",
    "                self.params[name].name = name\n",
    "                self.activs_list.append(name)\n",
    "        super().__setattr__(name, value)\n",
    "        \n",
    "    def __new__(cls):\n",
    "        obj = super().__new__(cls)\n",
    "        obj.params = {}\n",
    "        obj.layers_list = []\n",
    "        obj.activs_list = []\n",
    "        obj.sequence = []\n",
    "        return obj\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "class Optimizer:\n",
    "    def __init__(self, params=[], outputs=[], actives=['relu','sigmoid','softmax'], learning_rate=0.001):\n",
    "        self.params = params\n",
    "        self.outputs = outputs.copy()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activations = {}\n",
    "        for i in actives:\n",
    "            self.activations[i] = Activations(i)\n",
    "    \n",
    "    def qism_gradient(self, order, satr, ustun, batch):\n",
    "        if len(self.outputs[-1]) > 2:\n",
    "            if self.outputs[-1][2] == 'softmax':\n",
    "                if order == len(self.params) - 1:\n",
    "                    last_act = np.zeros((self.outputs[-1][0][batch].shape))\n",
    "                    last_act[ustun] = 1.0\n",
    "                    return last_act\n",
    "                else:\n",
    "                    last_act = 1\n",
    "            else:\n",
    "                last_act = self.activations[self.outputs[-1][-1]].derivate(self.outputs[-1][0][batch])\n",
    "        else:\n",
    "            last_act = 1\n",
    "        if order == len(self.params) - 1:\n",
    "            return last_act\n",
    "        else:\n",
    "            grad = self.params[order+1][0][[ustun]].copy()\n",
    "            for i in range(order+2, len(self.params)):\n",
    "                if len(self.outputs[i]) > 2:\n",
    "                    acts_matrix = np.tile(self.activations[self.outputs[i][-1]].derivate(self.outputs[i][0][batch])[0], (self.params[i][0].shape[1], 1)).T\n",
    "                else:\n",
    "                    acts_matrix = np.ones(self.params[i][0].shape)\n",
    "                layer_next = np.multiply(self.params[i][0], acts_matrix)\n",
    "                del acts_matrix\n",
    "                grad = np.dot(grad, layer_next)\n",
    "            result = np.multiply(grad, last_act)\n",
    "            del grad, last_act\n",
    "            return result\n",
    "    \n",
    "    def backward(self, target):\n",
    "        common_der_loss = self.loss.derivate(target, self.outputs[-1][0])\n",
    "        # layerlar uchun sikl\n",
    "        for i in range(len(self.params)-1, -1, -1):\n",
    "            ###################################\n",
    "            #### weightlarni update qilish ####\n",
    "            ###################################\n",
    "            qism_grad = []\n",
    "            if len(self.outputs[i]) == 3 and i != len(self.params)-1 and self.outputs[i][2] != 'softmax':\n",
    "                act_common = self.activations[self.outputs[i][-1]].derivate(self.outputs[i][0])\n",
    "                common = []\n",
    "                for son in range(len(act_common)):\n",
    "                    common.append(np.tile(act_common[son], (self.params[i][0].shape[1], 1)).T)\n",
    "                act_common = np.array(common)\n",
    "                del common\n",
    "            else:\n",
    "                act_common = np.ones((self.batch_size, self.params[i][0].shape[0], self.params[i][0].shape[1]))\n",
    "            for satr in range(len(self.params[i][0])):\n",
    "                for ustun in range(len(self.params[i][0][satr])):\n",
    "                    try:\n",
    "                        umumiy_grad = 0\n",
    "                        for batch in range(self.batch_size):\n",
    "                            umumiy_grad += np.dot(qism_grad[ustun][batch], common_der_loss[batch])*act_common[batch][satr, ustun]*self.outputs[i][0][batch][satr]\n",
    "                        self.params[i][0][satr,ustun] -= self.learning_rate*(umumiy_grad/self.batch_size)\n",
    "                        del umumiy_grad\n",
    "                    except:\n",
    "                        qism_grad.append([])\n",
    "                        umumiy_grad = 0\n",
    "                        for batch in range(self.batch_size):\n",
    "                            qism_grad[-1].append(self.qism_gradient(i, satr, ustun, batch))\n",
    "                            umumiy_grad += np.dot(qism_grad[ustun][batch], common_der_loss[batch])*act_common[batch][satr, ustun]*self.outputs[i][0][batch][satr]\n",
    "                        self.params[i][0][satr,ustun] -= self.learning_rate*(umumiy_grad/self.batch_size)\n",
    "                        del umumiy_grad\n",
    "            #################################\n",
    "            #### biaslarni update qilish ####\n",
    "            #################################\n",
    "            for bias in range(len(qism_grad)):\n",
    "                umumiy_grad = 0\n",
    "                for batch in range(self.batch_size):\n",
    "                    umumiy_grad += np.dot(qism_grad[ustun][batch], common_der_loss[batch])*act_common[batch][satr, ustun]\n",
    "                self.params[i][1][0,bias] -= self.learning_rate*(umumiy_grad/self.batch_size)\n",
    "                del umumiy_grad\n",
    "            del qism_grad\n",
    "        del common_der_loss\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def load_model(path):\n",
    "    model = PyDahoShoxa()\n",
    "    # read model parameters and architecture\n",
    "    with open(path) as f:\n",
    "        model_data = json.load(f)\n",
    "    model.sequence = model_data['Architecture']\n",
    "    # set layers and their parameters\n",
    "    lay = 1\n",
    "    for i in model.sequence:\n",
    "        if isinstance(i, list):\n",
    "            model.layers_list.append(i[2])\n",
    "            model.params[i[2]] = Layer(i[0], i[1])\n",
    "            model.params[i[2]].name = i[2]\n",
    "            model.params[i[2]].set_parameters(model_data[f'Layer{lay}'])\n",
    "            setattr(model, i[2], model.params[i[2]])\n",
    "            lay += 1\n",
    "        else:\n",
    "            model.activs_list.append(i)\n",
    "            model.params[i] = Activations(i)\n",
    "    model.compile(Optimizer(params=model.parameters(), learning_rate=model_data['Compile']['optimizer_learning_rate']), loss=model_data['Compile']['loss'])\n",
    "\n",
    "    def forward(x):\n",
    "        for i in model.sequence:\n",
    "            if isinstance(i, list):\n",
    "                x = model.params[i[2]](x)\n",
    "            else:\n",
    "                x = model.params[i](x)\n",
    "        return x\n",
    "    \n",
    "    model.forward = forward\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94577382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(PyDahoShoxa):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = Layer(784, 64)\n",
    "        self.l2 = Layer(64, 16)\n",
    "        self.l3 = Layer(16, 10)\n",
    "        self.relu = Activations('relu')\n",
    "        self.softmax = Activations('softmax')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.l1(x))\n",
    "        x = self.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "my_model = Model()\n",
    "my_model.compile(optimizer=Optimizer(params=my_model.parameters(), learning_rate=0.001), loss='cross_entropy_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def057d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9/10 [===========================>..] - ETA: 8s - loss: 188.2006 - accuracy: 0.02010\r"
     ]
    }
   ],
   "source": [
    "my_model.fit(trainx[:100], onehot[:100], batch_size=10, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e919d5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
